services:
  # Ollama - Local LLM inference engine
  ollama:
    image: ollama/ollama:latest
    container_name: ai-ollama
    restart: unless-stopped
    profiles: ["gpu"]
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./data/models:/models
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - web
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # AI Inference Proxy - Unified proxy for local and cloud models
  ai-inference-proxy:
    build: ./ai-inference-proxy
    container_name: ai-inference-proxy
    restart: unless-stopped
    ports:
      - "8192:8192"
    environment:
      - PROXY_PORT=8192
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENROUTER_API_URL=${OPENROUTER_API_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-*}
      - RATE_LIMIT=${RATE_LIMIT:-100}
    networks:
      - web
    # Note: ollama dependency is optional (has GPU profile)
    # ai-inference-proxy will work without it, just won't have local models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8192/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Open WebUI - Web interface for Ollama
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ai-openwebui
    restart: unless-stopped
    ports:
      - "8189:8080"
    volumes:
      - openwebui_data:/app/backend/data
      - ./data/cache:/app/backend/cache
    environment:
      # Local models via Ollama (shows in "All" filter, local models)
      - OLLAMA_BASE_URL=http://ollama:11434
      # Unified proxy for both local and cloud models (shows in "External" filter)
      # The unified proxy combines models from Ollama and OpenRouter
      - OPENAI_API_BASE_URL=http://ai-inference-proxy:8192
      - OPENAI_API_KEY=${OPENROUTER_API_KEY}
      # WebUI configuration
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_AUTH=${WEBUI_AUTH:-True}
      - WEBUI_NAME=${WEBUI_NAME:-Hephaestus AI}
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-user}
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-False}
    networks:
      - web
    depends_on:
      - ai-inference-proxy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ComfyUI - Advanced image generation workflows
  comfyui:
    build: ./comfyui
    container_name: ai-comfyui
    restart: unless-stopped
    profiles: ["gpu"]
    ports:
      - "8188:8188"
    volumes:
      - comfyui_data:/opt/ComfyUI/user
      - ./data/models:/opt/ComfyUI/models
      - ./data/outputs:/opt/ComfyUI/output
      - ./comfyui/workflows:/opt/ComfyUI/user/default/workflows
      - ./comfyui/custom_nodes:/opt/ComfyUI/custom_nodes
    environment:
      - CLI_ARGS=--listen 0.0.0.0 --port 8188
    networks:
      - web
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 120s

  # OpenRouter Proxy - Cloud model access
  openrouter-proxy:
    build: ./openrouter-proxy
    container_name: ai-openrouter-proxy
    restart: unless-stopped
    ports:
      - "8190:8190"
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - PROXY_PORT=8190
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-*}
      - RATE_LIMIT=${RATE_LIMIT:-100}
    networks:
      - web
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8190/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Model Manager - Download and manage AI models
  model-manager:
    build: ./model-manager
    container_name: ai-model-manager
    restart: unless-stopped
    profiles: ["gpu"]
    ports:
      - "8191:8191"
    volumes:
      - ./data/models:/app/models
      - ollama_data:/app/ollama_data
      - comfyui_data:/app/comfyui_data
      - ./model-manager/config:/app/config
    environment:
      - MANAGER_PORT=8191
      - OLLAMA_URL=http://ollama:11434
      - COMFYUI_URL=http://comfyui:8188
      - MODEL_STORAGE_PATH=/app/models
    networks:
      - web
    depends_on:
      - ollama
      - comfyui
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8191/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  ollama_data:
    driver: local
  openwebui_data:
    driver: local
  comfyui_data:
    driver: local

networks:
  web:
    external: true
    name: homelab-web
