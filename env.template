# Example environment configuration for AI Inference stack
# Copy this file to .env and adjust values as needed.

# Core Configuration
DOMAIN=chrislawrence.ca
SUBDOMAIN_PREFIX=ai

# Open WebUI
WEBUI_SECRET_KEY=change-me
WEBUI_AUTH=True
WEBUI_NAME=Hephaestus AI

# OpenRouter Integration
OPENROUTER_API_KEY=change-me
OPENROUTER_API_URL=https://openrouter.ai/api/v1

# Unified Inference Proxy
# DEFAULT_BACKEND determines which inference backend to use by default.
# Options:
#   auto  - automatically choose local if model is available, otherwise cloud
#   local - force local (Ollama) models
#   cloud - force cloud (OpenRouter) models
DEFAULT_BACKEND=auto

# GPU Configuration (already supported via Docker profiles)
# To enable GPU-powered local inference, start the stack with:
#   docker compose --profile gpu up -d
# This will start Ollama, ComfyUI, and the Model Manager with GPU access.
NVIDIA_VISIBLE_DEVICES=all
CUDA_VISIBLE_DEVICES=0
